{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of PersonAttrubutes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ankush2805/EIP/blob/master/Tests5/checked.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyq8CE4ug5BK",
        "colab_type": "code",
        "outputId": "56a4c518-5246-4dc0-feca-3064efb16ede",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# mount gdrive and unzip data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!unzip -q \"/content/gdrive/My Drive/hvc_data.zip\"\n",
        "# look for `hvc_annotations.csv` file and `resized` dir\n",
        "%ls "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n",
            "\u001b[0m\u001b[01;34mgdrive\u001b[0m/  hvc_annotations.csv  \u001b[01;34mresized\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYbNQzK6kj94",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b8c79930-4ade-40bd-9e6c-d15e63bd85e7"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import cv2\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path \n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "\n",
        "from keras.applications import VGG16\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Input, GlobalAveragePooling2D\n",
        "from keras.models import Model\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQkbSpLK4sTP",
        "colab_type": "code",
        "outputId": "532acab8-946a-4527-a778-c5bf23d014f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# load annotations\n",
        "df = pd.read_csv(\"hvc_annotations.csv\")\n",
        "del df[\"filename\"] # remove unwanted column\n",
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gender</th>\n",
              "      <th>imagequality</th>\n",
              "      <th>age</th>\n",
              "      <th>weight</th>\n",
              "      <th>carryingbag</th>\n",
              "      <th>footwear</th>\n",
              "      <th>emotion</th>\n",
              "      <th>bodypose</th>\n",
              "      <th>image_path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>male</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/1.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>female</td>\n",
              "      <td>Average</td>\n",
              "      <td>35-45</td>\n",
              "      <td>over-weight</td>\n",
              "      <td>None</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Angry/Serious</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Grocery/Home/Plastic Bag</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>male</td>\n",
              "      <td>Good</td>\n",
              "      <td>45-55</td>\n",
              "      <td>normal-healthy</td>\n",
              "      <td>Daily/Office/Work Bag</td>\n",
              "      <td>Normal</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>female</td>\n",
              "      <td>Good</td>\n",
              "      <td>35-45</td>\n",
              "      <td>slightly-overweight</td>\n",
              "      <td>None</td>\n",
              "      <td>CantSee</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>Front-Frontish</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   gender imagequality    age  ...        emotion        bodypose     image_path\n",
              "0    male      Average  35-45  ...        Neutral  Front-Frontish  resized/1.jpg\n",
              "1  female      Average  35-45  ...  Angry/Serious  Front-Frontish  resized/2.jpg\n",
              "2    male         Good  45-55  ...        Neutral  Front-Frontish  resized/3.jpg\n",
              "3    male         Good  45-55  ...        Neutral  Front-Frontish  resized/4.jpg\n",
              "4  female         Good  35-45  ...        Neutral  Front-Frontish  resized/5.jpg\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "202OJva345WA",
        "colab_type": "code",
        "outputId": "c4ea8192-ba23-4d58-e5b0-23d845f5687a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 917
        }
      },
      "source": [
        "# one hot encoding of labels\n",
        "\n",
        "one_hot_df = pd.concat([\n",
        "    df[[\"image_path\"]],\n",
        "    pd.get_dummies(df.gender, prefix=\"gender\"),\n",
        "    pd.get_dummies(df.imagequality, prefix=\"imagequality\"),\n",
        "    pd.get_dummies(df.age, prefix=\"age\"),\n",
        "    pd.get_dummies(df.weight, prefix=\"weight\"),\n",
        "    pd.get_dummies(df.carryingbag, prefix=\"carryingbag\"),\n",
        "    pd.get_dummies(df.footwear, prefix=\"footwear\"),\n",
        "    pd.get_dummies(df.emotion, prefix=\"emotion\"),\n",
        "    pd.get_dummies(df.bodypose, prefix=\"bodypose\"),\n",
        "], axis = 1)\n",
        "\n",
        "one_hot_df.head().T"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>image_path</th>\n",
              "      <td>resized/1.jpg</td>\n",
              "      <td>resized/2.jpg</td>\n",
              "      <td>resized/3.jpg</td>\n",
              "      <td>resized/4.jpg</td>\n",
              "      <td>resized/5.jpg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_female</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gender_male</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Average</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>imagequality_Good</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_15-25</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_25-35</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_35-45</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_45-55</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_55+</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_over-weight</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>weight_underweight</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>carryingbag_None</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>footwear_Normal</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Happy</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>emotion_Sad</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Back</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bodypose_Side</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  0  ...              4\n",
              "image_path                            resized/1.jpg  ...  resized/5.jpg\n",
              "gender_female                                     0  ...              1\n",
              "gender_male                                       1  ...              0\n",
              "imagequality_Average                              1  ...              0\n",
              "imagequality_Bad                                  0  ...              0\n",
              "imagequality_Good                                 0  ...              1\n",
              "age_15-25                                         0  ...              0\n",
              "age_25-35                                         0  ...              0\n",
              "age_35-45                                         1  ...              1\n",
              "age_45-55                                         0  ...              0\n",
              "age_55+                                           0  ...              0\n",
              "weight_normal-healthy                             1  ...              0\n",
              "weight_over-weight                                0  ...              0\n",
              "weight_slightly-overweight                        0  ...              1\n",
              "weight_underweight                                0  ...              0\n",
              "carryingbag_Daily/Office/Work Bag                 0  ...              0\n",
              "carryingbag_Grocery/Home/Plastic Bag              1  ...              0\n",
              "carryingbag_None                                  0  ...              1\n",
              "footwear_CantSee                                  0  ...              1\n",
              "footwear_Fancy                                    0  ...              0\n",
              "footwear_Normal                                   1  ...              0\n",
              "emotion_Angry/Serious                             0  ...              0\n",
              "emotion_Happy                                     0  ...              0\n",
              "emotion_Neutral                                   1  ...              1\n",
              "emotion_Sad                                       0  ...              0\n",
              "bodypose_Back                                     0  ...              0\n",
              "bodypose_Front-Frontish                           1  ...              1\n",
              "bodypose_Side                                     0  ...              0\n",
              "\n",
              "[28 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPeulaj2-S1k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_random_eraser(input_img, p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=1, pixel_level=False):\n",
        "  img_h, img_w, img_c = input_img.shape\n",
        "  p_1 = np.random.rand()\n",
        "  if p_1 > p:\n",
        "    return input_img\n",
        "\n",
        "  while True:\n",
        "    s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "    r = np.random.uniform(r_1, r_2)\n",
        "    w = int(np.sqrt(s / r))\n",
        "    h = int(np.sqrt(s * r))\n",
        "    left = np.random.randint(0, img_w)\n",
        "    top = np.random.randint(0, img_h)\n",
        "\n",
        "    if left + w <= img_w and top + h <= img_h:\n",
        "      break\n",
        "\n",
        "    if pixel_level:\n",
        "      c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "    else:\n",
        "      c = np.random.uniform(v_l, v_h)\n",
        "    input_img[top:top + h, left:left + w, :] = c\n",
        "  return input_img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxYtEYa99vvM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def augmentor(images):\n",
        "\t\t'Apply data augmentation'\n",
        "\t\tsometimes = lambda aug: iaa.Sometimes(0.5, aug)\n",
        "\t\tseq = iaa.Sequential(\n",
        "\t\t\t\t[\n",
        "\t\t\t\t# apply the following augmenters to most images\n",
        "\t\t\t\tiaa.Fliplr(0.3),  # horizontally flip 50% of all images\n",
        "\t\t\t\tsometimes(iaa.Affine(\n",
        "\t\t\t\t\tscale={\"x\": (0.9, 1.1), \"y\": (0.9, 1.1)},\n",
        "\t\t\t\t\t# scale images to 80-120% of their size, individually per axis\n",
        "\t\t\t\t\ttranslate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)},\n",
        "\t\t\t\t\t# translate by -20 to +20 percent (per axis)\n",
        "\t\t\t\t\trotate=(-10, 10),  # rotate by -45 to +45 degrees\n",
        "\t\t\t\t\tshear=(-5, 5),  # shear by -16 to +16 degrees\n",
        "\t\t\t\t\torder=[0, 1],\n",
        "\t\t\t\t\t# use nearest neighbour or bilinear interpolation (fast)\n",
        "\t\t\t\t\tcval=(0, 255),  # if mode is constant, use a cval between 0 and 255\n",
        "\t\t\t\t\tmode=ia.ALL\n",
        "\t\t\t\t\t# use any of scikit-image's warping modes (see 2nd image from the top for examples)\n",
        "\t\t\t\t)),\n",
        "\t\t\t\t# execute 0 to 5 of the following (less important) augmenters per image\n",
        "\t\t\t\t# don't execute all of them, as that would often be way too strong\n",
        "\t\t\t\tiaa.SomeOf((0, 5),\n",
        "\t\t\t\t           [sometimes(iaa.Superpixels(p_replace=(0, 1.0),\n",
        "\t\t\t\t\t\t                                     n_segments=(20, 200))),\n",
        "\t\t\t\t\t           # convert images into their superpixel representation\n",
        "\t\t\t\t\t           iaa.OneOf([\n",
        "\t\t\t\t\t\t\t           iaa.GaussianBlur((0, 1.0)),\n",
        "\t\t\t\t\t\t\t           # blur images with a sigma between 0 and 3.0\n",
        "\t\t\t\t\t\t\t           iaa.AverageBlur(k=(3, 5)),\n",
        "\t\t\t\t\t\t\t           # blur image using local means with kernel sizes between 2 and 7\n",
        "\t\t\t\t\t\t\t           iaa.MedianBlur(k=(3, 5)),\n",
        "\t\t\t\t\t\t\t           # blur image using local medians with kernel sizes between 2 and 7\n",
        "\t\t\t\t\t           ]),\n",
        "\t\t\t\t\t           iaa.Sharpen(alpha=(0, 1.0), lightness=(0.9, 1.1)),\n",
        "\t\t\t\t\t           # sharpen images\n",
        "\t\t\t\t\t           iaa.Emboss(alpha=(0, 1.0), strength=(0, 2.0)),\n",
        "\t\t\t\t\t           # emboss images\n",
        "\t\t\t\t\t           # search either for all edges or for directed edges,\n",
        "\t\t\t\t\t           # blend the result with the original image using a blobby mask\n",
        "\t\t\t\t\t           iaa.SimplexNoiseAlpha(iaa.OneOf([\n",
        "\t\t\t\t\t\t\t           iaa.EdgeDetect(alpha=(0.5, 1.0)),\n",
        "\t\t\t\t\t\t\t           iaa.DirectedEdgeDetect(alpha=(0.5, 1.0),\n",
        "\t\t\t\t\t\t\t                                  direction=(0.0, 1.0)),\n",
        "\t\t\t\t\t           ])),\n",
        "\t\t\t\t\t           iaa.AdditiveGaussianNoise(loc=0,\n",
        "\t\t\t\t\t                                     scale=(0.0, 0.01 * 255),\n",
        "\t\t\t\t\t                                     per_channel=0.5),\n",
        "\t\t\t\t\t           # add gaussian noise to images\n",
        "\t\t\t\t\t           \n",
        "\t\t\t\t\t           \n",
        "\t\t\t\t\t           iaa.Add((-2, 2), per_channel=0.5),\n",
        "\t\t\t\t\t           # change brightness of images (by -10 to 10 of original value)\n",
        "\t\t\t\t\t           iaa.AddToHueAndSaturation((-1, 1)),\n",
        "\t\t\t\t\t           # change hue and saturation\n",
        "\t\t\t\t\t           # either change the brightness of the whole image (sometimes\n",
        "\t\t\t\t\t           # per channel) or change the brightness of subareas\n",
        "\t\t\t\t\t           iaa.OneOf([\n",
        "\t\t\t\t\t\t\t           iaa.Multiply((0.9, 1.1), per_channel=0.5),\n",
        "\t\t\t\t\t\t\t           iaa.FrequencyNoiseAlpha(\n",
        "\t\t\t\t\t\t\t\t\t           exponent=(-1, 0),\n",
        "\t\t\t\t\t\t\t\t\t           first=iaa.Multiply((0.9, 1.1),\n",
        "\t\t\t\t\t\t\t\t\t                              per_channel=True),\n",
        "\t\t\t\t\t\t\t\t\t           second=iaa.ContrastNormalization(\n",
        "\t\t\t\t\t\t\t\t\t\t\t           (0.9, 1.1))\n",
        "\t\t\t\t\t\t\t           )\n",
        "\t\t\t\t\t           ]),\n",
        "\t\t\t\t\t           sometimes(iaa.ElasticTransformation(alpha=(0.5, 3.5),\n",
        "\t\t\t\t\t                                               sigma=0.25)),\n",
        "\t\t\t\t\t           # move pixels locally around (with random strengths)\n",
        "\t\t\t\t\t           sometimes(iaa.PiecewiseAffine(scale=(0.01, 0.05))),\n",
        "\t\t\t\t\t           # sometimes move parts of the image around\n",
        "\t\t\t\t\t           sometimes(iaa.PerspectiveTransform(scale=(0.01, 0.1)))\n",
        "\t\t\t\t           ],\n",
        "\t\t\t\t           random_order=True\n",
        "\t\t\t\t           )\n",
        "\t\t\t\t],\n",
        "\t\t\t\trandom_order=True\n",
        "\t\t)\n",
        "\t\treturn seq.augment_images(images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9SSZBFQZ5El",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "from albumentations import (\n",
        "    Compose, HorizontalFlip, CLAHE, HueSaturationValue,\n",
        "    RandomBrightness, RandomContrast, RandomGamma,\n",
        "    ToFloat, ShiftScaleRotate, Normalize\n",
        ")\n",
        "\n",
        "AUGMENTATIONS_TRAIN = Compose([                        \n",
        "    HorizontalFlip(p=0.5),\n",
        "    RandomContrast(limit=0.2, p=0.5),\n",
        "    RandomGamma(gamma_limit=(80, 120), p=0.5),\n",
        "    RandomBrightness(limit=0.2, p=0.5),\n",
        "    HueSaturationValue(hue_shift_limit=5, sat_shift_limit=20,\n",
        "                       val_shift_limit=10, p=.9),\n",
        "     CLAHE(p=1.0, clip_limit=2.0),\n",
        "    ShiftScaleRotate(\n",
        "        shift_limit=0.0625, scale_limit=0.1, \n",
        "        rotate_limit=15, border_mode=cv2.BORDER_REFLECT_101, p=0.8), \n",
        "    ToFloat(max_value=255)\n",
        "])\n",
        "\n",
        "AUGMENTATIONS_TEST = Compose([\n",
        "    ToFloat(max_value=255)\n",
        "])\n",
        "\n",
        "AUGMENTATIONS_NORMALIZE = Compose([\n",
        "    ToFloat(max_value=255)\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ll94zTv6w5i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "14fc1b26-6611-4fd5-d2b6-31891faf4ce0"
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "import imgaug as ia\n",
        "import imgaug.augmenters as iaa\n",
        "\n",
        "ia.seed(1)\n",
        "\n",
        "# Label columns per attribute\n",
        "_gender_cols_ = [col for col in one_hot_df.columns if col.startswith(\"gender\")]\n",
        "_imagequality_cols_ = [col for col in one_hot_df.columns if col.startswith(\"imagequality\")]\n",
        "_age_cols_ = [col for col in one_hot_df.columns if col.startswith(\"age\")]\n",
        "_weight_cols_ = [col for col in one_hot_df.columns if col.startswith(\"weight\")]\n",
        "_carryingbag_cols_ = [col for col in one_hot_df.columns if col.startswith(\"carryingbag\")]\n",
        "_footwear_cols_ = [col for col in one_hot_df.columns if col.startswith(\"footwear\")]\n",
        "_emotion_cols_ = [col for col in one_hot_df.columns if col.startswith(\"emotion\")]\n",
        "_bodypose_cols_ = [col for col in one_hot_df.columns if col.startswith(\"bodypose\")]\n",
        "\n",
        "class PersonDataGenerator(keras.utils.Sequence):\n",
        "    \"\"\"Ground truth data generator\"\"\"\n",
        "\n",
        "    \n",
        "    def __init__(self, df, batch_size=32, shuffle=True, augmentations=AUGMENTATIONS_TEST):\n",
        "        self.df = df\n",
        "        self.batch_size=batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "        self.augment = augmentations\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(self.df.shape[0] / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"fetch batched images and targets\"\"\"\n",
        "        batch_slice = slice(index * self.batch_size, (index + 1) * self.batch_size)\n",
        "        items = self.df.iloc[batch_slice]\n",
        "        image = np.stack([cv2.imread(item[\"image_path\"]) for _, item in items.iterrows()])\n",
        "\n",
        "        image = np.stack([AUGMENTATIONS_NORMALIZE(image=img)[\"image\"] for img in image])\n",
        "\n",
        "        if self.batch_size ==32:\n",
        "          image = np.stack([get_random_eraser(img) for img in image])\n",
        "          image = augmentor(image)\n",
        "        target = {\n",
        "            \"gender_output\": items[_gender_cols_].values,\n",
        "            \"image_quality_output\": items[_imagequality_cols_].values,\n",
        "            \"age_output\": items[_age_cols_].values,\n",
        "            \"weight_output\": items[_weight_cols_].values,\n",
        "            \"bag_output\": items[_carryingbag_cols_].values,\n",
        "            \"pose_output\": items[_bodypose_cols_].values,\n",
        "            \"footwear_output\": items[_footwear_cols_].values,\n",
        "            \"emotion_output\": items[_emotion_cols_].values,\n",
        "        }\n",
        "        return image, target\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Updates indexes after each epoch\"\"\"\n",
        "        if self.shuffle == True:\n",
        "            self.df = self.df.sample(frac=1).reset_index(drop=True)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-2b1387346dff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Label columns per attribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0m_gender_cols_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mone_hot_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gender\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0m_imagequality_cols_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mone_hot_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"imagequality\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0m_age_cols_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mone_hot_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"age\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'one_hot_df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVE8-OaZ8J5q",
        "colab_type": "code",
        "outputId": "9d21afe3-7b34-4a4e-d138-cf4ae34e122d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_df, val_df = train_test_split(one_hot_df, test_size=0.15)\n",
        "train_df.shape, val_df.shape"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((11537, 28), (2036, 28))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5m15DLyF2ot",
        "colab_type": "code",
        "outputId": "1c3d63db-6ec3-41cf-9460-9a66127546f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "train_df.head()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_path</th>\n",
              "      <th>gender_female</th>\n",
              "      <th>gender_male</th>\n",
              "      <th>imagequality_Average</th>\n",
              "      <th>imagequality_Bad</th>\n",
              "      <th>imagequality_Good</th>\n",
              "      <th>age_15-25</th>\n",
              "      <th>age_25-35</th>\n",
              "      <th>age_35-45</th>\n",
              "      <th>age_45-55</th>\n",
              "      <th>age_55+</th>\n",
              "      <th>weight_normal-healthy</th>\n",
              "      <th>weight_over-weight</th>\n",
              "      <th>weight_slightly-overweight</th>\n",
              "      <th>weight_underweight</th>\n",
              "      <th>carryingbag_Daily/Office/Work Bag</th>\n",
              "      <th>carryingbag_Grocery/Home/Plastic Bag</th>\n",
              "      <th>carryingbag_None</th>\n",
              "      <th>footwear_CantSee</th>\n",
              "      <th>footwear_Fancy</th>\n",
              "      <th>footwear_Normal</th>\n",
              "      <th>emotion_Angry/Serious</th>\n",
              "      <th>emotion_Happy</th>\n",
              "      <th>emotion_Neutral</th>\n",
              "      <th>emotion_Sad</th>\n",
              "      <th>bodypose_Back</th>\n",
              "      <th>bodypose_Front-Frontish</th>\n",
              "      <th>bodypose_Side</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>10204</th>\n",
              "      <td>resized/10206.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>942</th>\n",
              "      <td>resized/943.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2491</th>\n",
              "      <td>resized/2492.jpg</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4229</th>\n",
              "      <td>resized/4230.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5589</th>\n",
              "      <td>resized/5590.jpg</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              image_path  gender_female  ...  bodypose_Front-Frontish  bodypose_Side\n",
              "10204  resized/10206.jpg              0  ...                        1              0\n",
              "942      resized/943.jpg              1  ...                        1              0\n",
              "2491    resized/2492.jpg              0  ...                        0              1\n",
              "4229    resized/4230.jpg              1  ...                        0              0\n",
              "5589    resized/5590.jpg              1  ...                        0              0\n",
              "\n",
              "[5 rows x 28 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTiOi5tVBnhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create train and validation data generators\n",
        "train_gen = PersonDataGenerator(train_df, batch_size=32)\n",
        "valid_gen = PersonDataGenerator(val_df, batch_size=64, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pMDGat-Ghow",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "78a207d0-0a53-40cf-e2ac-ae9b8a77f8de"
      },
      "source": [
        "# get number of output units from data\n",
        "images, targets = next(iter(train_gen))\n",
        "num_units = { k.split(\"_output\")[0]:v.shape[1] for k, v in targets.items()}\n",
        "num_units"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'age': 5,\n",
              " 'bag': 3,\n",
              " 'emotion': 4,\n",
              " 'footwear': 3,\n",
              " 'gender': 2,\n",
              " 'image_quality': 3,\n",
              " 'pose': 3,\n",
              " 'weight': 4}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bec4F2M-8u7l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "    \"\"\"Learning Rate Schedule\n",
        "\n",
        "    Learning rate is scheduled to be reduced after 80, 120, 160, 180 epochs.\n",
        "    Called automatically every epoch as part of callbacks during training.\n",
        "\n",
        "    # Arguments\n",
        "        epoch (int): The number of epochs\n",
        "\n",
        "    # Returns\n",
        "        lr (float32): learning rate\n",
        "    \"\"\"\n",
        "    lr = 0.001\n",
        "    if epoch > 95:\n",
        "        lr = 0.000001\n",
        "    if epoch > 80:\n",
        "        lr = 0.00005\n",
        "    elif epoch > 60:\n",
        "        lr = 0.0001\n",
        "    elif epoch > 40:\n",
        "        lr = 0.0005\n",
        "    elif epoch > 20:\n",
        "        lr = 0.001\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03W8Pagg_Ppp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "334648c3-66f8-4443-83f3-4fa532dd4a95"
      },
      "source": [
        "from keras.applications import ResNet101V2, ResNet50, InceptionV3, ResNet152V2, InceptionResNetV2, DenseNet201\n",
        "backbone = ResNet152V2(\n",
        "    weights=None, \n",
        "    include_top=False,\n",
        "    input_tensor=Input(shape=(224, 224, 3))\n",
        ")\n",
        "\n",
        "neck = backbone.output\n",
        "#neck = Flatten(name=\"flatten\")(neck)\n",
        "neck = GlobalAveragePooling2D()(neck)\n",
        "neck = Dense(512, activation=\"relu\")(neck)\n",
        "\n",
        "def build_dense_tower(in_layer):\n",
        "    neck = Dense(128, activation=\"relu\")(in_layer)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    return neck\n",
        "\n",
        "def build_tower(in_layer):\n",
        "    #neck = Dropout(0.2)(in_layer)\n",
        "    neck = Dense(128, activation=\"relu\")(in_layer)\n",
        "    #neck = Dropout(0.3)(in_layer)\n",
        "    neck = Dense(128, activation=\"relu\")(neck)\n",
        "    return neck\n",
        "\n",
        "\n",
        "def build_head(name, in_layer):\n",
        "    return  Dense(\n",
        "        num_units[name], activation=\"softmax\", name=f\"{name}_output\"\n",
        "    )(in_layer)\n",
        "\n",
        "# heads\n",
        "gender = build_head(\"gender\", build_dense_tower(neck))\n",
        "image_quality = build_head(\"image_quality\", build_dense_tower(neck))\n",
        "age = build_head(\"age\", build_tower(neck))\n",
        "weight = build_head(\"weight\", build_tower(neck))\n",
        "bag = build_head(\"bag\", build_dense_tower(neck))\n",
        "footwear = build_head(\"footwear\", build_dense_tower(neck))\n",
        "emotion = build_head(\"emotion\", build_tower(neck))\n",
        "pose = build_head(\"pose\", build_dense_tower(neck))\n",
        "\n",
        "\n",
        "model = Model(\n",
        "    inputs=backbone.input, \n",
        "    outputs=[gender, image_quality, age, weight, bag, footwear, pose, emotion]\n",
        ")\n",
        "#from keras.models import load_model\n",
        "#model = load_model('/content/gdrive/My Drive/model_011.hdf5')"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfPG9C2eA1zn",
        "colab_type": "code",
        "outputId": "8ef28571-bfaa-4f76-c196-c60820d28178",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# losses = {\n",
        "# \t\"gender_output\": \"binary_crossentropy\",\n",
        "# \t\"image_quality_output\": \"categorical_crossentropy\",\n",
        "# \t\"age_output\": \"categorical_crossentropy\",\n",
        "# \t\"weight_output\": \"categorical_crossentropy\",\n",
        "\n",
        "# }\n",
        "# loss_weights = {\"gender_output\": 1.0, \"image_quality_output\": 1.0, \"age_output\": 1.0}\n",
        "from keras.optimizers import Adam\n",
        "opt = Adam(lr=lr_schedule(0))\n",
        "model.compile(\n",
        "    optimizer=opt,\n",
        "    loss=\"categorical_crossentropy\", \n",
        "    # loss_weights=loss_weights, \n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning rate:  0.001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zw2ZRIQ7BW-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_model_history(model_history):\n",
        "    acc = ['gender_output_acc', 'image_quality_output_acc' , 'age_output_acc', 'weight_output_acc','bag_output_acc', 'footwear_output_acc','pose_output_acc',  'emotion_output_acc']\n",
        "    val_acc = ['val_gender_output_acc', 'val_image_quality_output_acc' , 'val_age_output_acc', 'val_weight_output_acc','val_bag_output_acc', 'val_footwear_output_acc','val_pose_output_acc',  'val_emotion_output_acc']\n",
        "    loss = ['gender_output_loss', 'image_quality_output_loss' , 'age_output_loss', 'weight_output_loss','bag_output_loss', 'footwear_output_loss','pose_output_loss',  'emotion_output_loss']\n",
        "    val_loss = ['val_gender_output_loss', 'val_image_quality_output_loss' , 'val_age_output_loss', 'val_weight_output_loss','val_bag_output_loss', 'val_footwear_output_loss','val_pose_output_loss',  'val_emotion_output_loss']\n",
        "    fig, axs = plt.subplots(8,2,figsize=(25,25))\n",
        "    for i in range(8) :\n",
        "      axs[i][0].plot(range(1,len(model_history.history[acc[i]])+1),model_history.history[acc[i]])\n",
        "      axs[i][0].plot(range(1,len(model_history.history[val_acc[i]])+1),model_history.history[val_acc[i]])\n",
        "      axs[i][0].set_title(acc[i])\n",
        "      \n",
        "      axs[i][0].set_xticks(np.arange(1,len(model_history.history[val_acc[i]])+1),len(model_history.history[val_acc[i]])/10)\n",
        "      axs[i][0].legend(['train', 'val'], loc='best')\n",
        "      # summarize history for loss\n",
        "      axs[i][1].plot(range(1,len(model_history.history[loss[i]])+1),model_history.history[loss[i]])\n",
        "      axs[i][1].plot(range(1,len(model_history.history[val_loss[i]])+1),model_history.history[val_loss[i]])\n",
        "      axs[i][1].set_title(loss[i])\n",
        "      \n",
        "      axs[i][1].set_xticks(np.arange(1,len(model_history.history[loss[i]])+1),len(model_history.history[loss[i]])/10)\n",
        "      axs[i][1].legend(['train', 'val'], loc='best')\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpxv41EyNmN4",
        "colab_type": "code",
        "outputId": "68d21bc9-8e71-45d3-d250-6e35eece9479",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import os\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "#drive.mount('/content/gdrive')\n",
        "filepath = '/content/gdrive/My Drive/person_attributes_densenet_augment.hdf5'\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             verbose=1,save_weights_only=False,\n",
        "                             period =10)\n",
        "\n",
        "callbacks = [checkpoint]\n",
        "\n",
        "model_info = model.fit_generator(\n",
        "    generator=train_gen,\n",
        "    validation_data=valid_gen,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6, \n",
        "    epochs=100,\n",
        "    verbose=1, callbacks=callbacks\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "plot_model_history(model_info)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "360/360 [==============================] - 185s 514ms/step - loss: 11.6069 - gender_output_loss: 1.0289 - image_quality_output_loss: 1.4111 - age_output_loss: 2.1085 - weight_output_loss: 1.5888 - bag_output_loss: 1.4081 - footwear_output_loss: 1.5502 - pose_output_loss: 1.2821 - emotion_output_loss: 1.2292 - gender_output_acc: 0.5457 - image_quality_output_acc: 0.5400 - age_output_acc: 0.3822 - weight_output_acc: 0.6161 - bag_output_acc: 0.5460 - footwear_output_acc: 0.4734 - pose_output_acc: 0.6082 - emotion_output_acc: 0.7082 - val_loss: 60.1515 - val_gender_output_loss: 6.8695 - val_image_quality_output_loss: 8.6800 - val_age_output_loss: 12.1887 - val_weight_output_loss: 5.7568 - val_bag_output_loss: 7.2686 - val_footwear_output_loss: 8.8863 - val_pose_output_loss: 6.0017 - val_emotion_output_loss: 4.5000 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.4582 - val_age_output_acc: 0.2399 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4456 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "Epoch 2/100\n",
            "360/360 [==============================] - 152s 422ms/step - loss: 11.1654 - gender_output_loss: 1.0352 - image_quality_output_loss: 1.3755 - age_output_loss: 2.0316 - weight_output_loss: 1.3883 - bag_output_loss: 1.3171 - footwear_output_loss: 1.4589 - pose_output_loss: 1.2805 - emotion_output_loss: 1.2784 - gender_output_acc: 0.5500 - image_quality_output_acc: 0.5496 - age_output_acc: 0.3878 - weight_output_acc: 0.6290 - bag_output_acc: 0.5604 - footwear_output_acc: 0.5153 - pose_output_acc: 0.6120 - emotion_output_acc: 0.7030 - val_loss: 70.1105 - val_gender_output_loss: 6.9217 - val_image_quality_output_loss: 7.2873 - val_age_output_loss: 12.2592 - val_weight_output_loss: 5.7843 - val_bag_output_loss: 7.2873 - val_footwear_output_loss: 10.1144 - val_pose_output_loss: 6.0443 - val_emotion_output_loss: 14.4120 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.2394 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.3725 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.1058\n",
            "Epoch 3/100\n",
            "360/360 [==============================] - 152s 422ms/step - loss: 11.5568 - gender_output_loss: 1.1493 - image_quality_output_loss: 1.3899 - age_output_loss: 2.0543 - weight_output_loss: 1.2997 - bag_output_loss: 1.3150 - footwear_output_loss: 1.5328 - pose_output_loss: 1.4784 - emotion_output_loss: 1.3373 - gender_output_acc: 0.5488 - image_quality_output_acc: 0.5514 - age_output_acc: 0.3888 - weight_output_acc: 0.6343 - bag_output_acc: 0.5601 - footwear_output_acc: 0.4738 - pose_output_acc: 0.6003 - emotion_output_acc: 0.6992 - val_loss: 45.6326 - val_gender_output_loss: 7.0793 - val_image_quality_output_loss: 5.3403 - val_age_output_loss: 8.7671 - val_weight_output_loss: 4.3736 - val_bag_output_loss: 5.1301 - val_footwear_output_loss: 7.0706 - val_pose_output_loss: 4.3871 - val_emotion_output_loss: 3.4844 - val_gender_output_acc: 0.4068 - val_image_quality_output_acc: 0.5469 - val_age_output_acc: 0.2984 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5474 - val_footwear_output_acc: 0.4073 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "Epoch 4/100\n",
            "359/360 [============================>.] - ETA: 0s - loss: 10.3585 - gender_output_loss: 1.1419 - image_quality_output_loss: 1.3099 - age_output_loss: 1.8991 - weight_output_loss: 1.2325 - bag_output_loss: 1.1924 - footwear_output_loss: 1.3495 - pose_output_loss: 1.1449 - emotion_output_loss: 1.0883 - gender_output_acc: 0.5439 - image_quality_output_acc: 0.5504 - age_output_acc: 0.3932 - weight_output_acc: 0.6341 - bag_output_acc: 0.5636 - footwear_output_acc: 0.4516 - pose_output_acc: 0.6161 - emotion_output_acc: 0.7111\n",
            "360/360 [==============================] - 152s 423ms/step - loss: 10.3585 - gender_output_loss: 1.1406 - image_quality_output_loss: 1.3103 - age_output_loss: 1.8989 - weight_output_loss: 1.2343 - bag_output_loss: 1.1914 - footwear_output_loss: 1.3497 - pose_output_loss: 1.1452 - emotion_output_loss: 1.0881 - gender_output_acc: 0.5437 - image_quality_output_acc: 0.5502 - age_output_acc: 0.3933 - weight_output_acc: 0.6341 - bag_output_acc: 0.5636 - footwear_output_acc: 0.4517 - pose_output_acc: 0.6163 - emotion_output_acc: 0.7109 - val_loss: 10.2527 - val_gender_output_loss: 1.1455 - val_image_quality_output_loss: 1.2692 - val_age_output_loss: 1.8726 - val_weight_output_loss: 1.2569 - val_bag_output_loss: 1.1204 - val_footwear_output_loss: 1.3492 - val_pose_output_loss: 1.1637 - val_emotion_output_loss: 1.0754 - val_gender_output_acc: 0.5524 - val_image_quality_output_acc: 0.5464 - val_age_output_acc: 0.4022 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5418 - val_footwear_output_acc: 0.4521 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "Epoch 5/100\n",
            "360/360 [==============================] - 151s 419ms/step - loss: 10.1277 - gender_output_loss: 1.0135 - image_quality_output_loss: 1.3013 - age_output_loss: 1.8725 - weight_output_loss: 1.1739 - bag_output_loss: 1.1598 - footwear_output_loss: 1.3603 - pose_output_loss: 1.1454 - emotion_output_loss: 1.1009 - gender_output_acc: 0.5585 - image_quality_output_acc: 0.5523 - age_output_acc: 0.3913 - weight_output_acc: 0.6351 - bag_output_acc: 0.5653 - footwear_output_acc: 0.4483 - pose_output_acc: 0.6164 - emotion_output_acc: 0.7110 - val_loss: 7.9300 - val_gender_output_loss: 0.6908 - val_image_quality_output_loss: 0.9982 - val_age_output_loss: 1.4414 - val_weight_output_loss: 0.9854 - val_bag_output_loss: 0.9407 - val_footwear_output_loss: 1.0537 - val_pose_output_loss: 0.9256 - val_emotion_output_loss: 0.8942 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4047 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4446 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "Epoch 6/100\n",
            "360/360 [==============================] - 150s 416ms/step - loss: 10.1955 - gender_output_loss: 1.0030 - image_quality_output_loss: 1.2861 - age_output_loss: 1.9030 - weight_output_loss: 1.2162 - bag_output_loss: 1.2009 - footwear_output_loss: 1.3367 - pose_output_loss: 1.1720 - emotion_output_loss: 1.0775 - gender_output_acc: 0.5621 - image_quality_output_acc: 0.5540 - age_output_acc: 0.3934 - weight_output_acc: 0.6346 - bag_output_acc: 0.5655 - footwear_output_acc: 0.4546 - pose_output_acc: 0.6161 - emotion_output_acc: 0.7108 - val_loss: 58.9018 - val_gender_output_loss: 6.7692 - val_image_quality_output_loss: 7.0657 - val_age_output_loss: 11.9627 - val_weight_output_loss: 5.6291 - val_bag_output_loss: 7.1181 - val_footwear_output_loss: 10.0554 - val_pose_output_loss: 5.9148 - val_emotion_output_loss: 4.3868 - val_gender_output_acc: 0.5660 - val_image_quality_output_acc: 0.5459 - val_age_output_acc: 0.2419 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.3538 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7147\n",
            "Epoch 7/100\n",
            "360/360 [==============================] - 150s 417ms/step - loss: 10.0115 - gender_output_loss: 0.9488 - image_quality_output_loss: 1.2600 - age_output_loss: 1.8884 - weight_output_loss: 1.1524 - bag_output_loss: 1.1385 - footwear_output_loss: 1.3390 - pose_output_loss: 1.1678 - emotion_output_loss: 1.1167 - gender_output_acc: 0.5615 - image_quality_output_acc: 0.5537 - age_output_acc: 0.3903 - weight_output_acc: 0.6351 - bag_output_acc: 0.5658 - footwear_output_acc: 0.4504 - pose_output_acc: 0.6167 - emotion_output_acc: 0.7091 - val_loss: 60.1082 - val_gender_output_loss: 6.9081 - val_image_quality_output_loss: 7.2737 - val_age_output_loss: 12.2384 - val_weight_output_loss: 5.7630 - val_bag_output_loss: 7.2810 - val_footwear_output_loss: 10.1003 - val_pose_output_loss: 6.0225 - val_emotion_output_loss: 4.5212 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.2394 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5474 - val_footwear_output_acc: 0.3720 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "Epoch 8/100\n",
            "359/360 [============================>.] - ETA: 0s - loss: 9.7552 - gender_output_loss: 0.8848 - image_quality_output_loss: 1.2187 - age_output_loss: 1.8299 - weight_output_loss: 1.1877 - bag_output_loss: 1.1498 - footwear_output_loss: 1.3310 - pose_output_loss: 1.0967 - emotion_output_loss: 1.0566 - gender_output_acc: 0.5612 - image_quality_output_acc: 0.5539 - age_output_acc: 0.3935 - weight_output_acc: 0.6350 - bag_output_acc: 0.5658 - footwear_output_acc: 0.4473 - pose_output_acc: 0.6168 - emotion_output_acc: 0.7107Epoch 8/100\n",
            "360/360 [==============================] - 150s 417ms/step - loss: 9.7542 - gender_output_loss: 0.8842 - image_quality_output_loss: 1.2193 - age_output_loss: 1.8297 - weight_output_loss: 1.1868 - bag_output_loss: 1.1490 - footwear_output_loss: 1.3315 - pose_output_loss: 1.0963 - emotion_output_loss: 1.0573 - gender_output_acc: 0.5610 - image_quality_output_acc: 0.5539 - age_output_acc: 0.3937 - weight_output_acc: 0.6352 - bag_output_acc: 0.5658 - footwear_output_acc: 0.4471 - pose_output_acc: 0.6168 - emotion_output_acc: 0.7107 - val_loss: 7.8629 - val_gender_output_loss: 0.6834 - val_image_quality_output_loss: 0.9922 - val_age_output_loss: 1.4245 - val_weight_output_loss: 0.9855 - val_bag_output_loss: 0.9271 - val_footwear_output_loss: 1.0383 - val_pose_output_loss: 0.9186 - val_emotion_output_loss: 0.8933 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4052 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4456 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "Epoch 9/100\n",
            "360/360 [==============================] - 151s 420ms/step - loss: 9.6938 - gender_output_loss: 0.9141 - image_quality_output_loss: 1.1859 - age_output_loss: 1.7994 - weight_output_loss: 1.1684 - bag_output_loss: 1.1198 - footwear_output_loss: 1.3597 - pose_output_loss: 1.1221 - emotion_output_loss: 1.0244 - gender_output_acc: 0.5612 - image_quality_output_acc: 0.5534 - age_output_acc: 0.3932 - weight_output_acc: 0.6352 - bag_output_acc: 0.5656 - footwear_output_acc: 0.4420 - pose_output_acc: 0.6163 - emotion_output_acc: 0.7106 - val_loss: 7.8639 - val_gender_output_loss: 0.6832 - val_image_quality_output_loss: 0.9906 - val_age_output_loss: 1.4251 - val_weight_output_loss: 0.9863 - val_bag_output_loss: 0.9263 - val_footwear_output_loss: 1.0386 - val_pose_output_loss: 0.9184 - val_emotion_output_loss: 0.8954 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4052 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4456 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "Epoch 10/100\n",
            "360/360 [==============================] - 150s 418ms/step - loss: 7.8760 - gender_output_loss: 0.6858 - image_quality_output_loss: 0.9822 - age_output_loss: 1.4303 - weight_output_loss: 0.9811 - bag_output_loss: 0.9182 - footwear_output_loss: 1.0413 - pose_output_loss: 0.9275 - emotion_output_loss: 0.9096 - gender_output_acc: 0.5609 - image_quality_output_acc: 0.5543 - age_output_acc: 0.3977 - weight_output_acc: 0.6356 - bag_output_acc: 0.5657 - footwear_output_acc: 0.4448 - pose_output_acc: 0.6169 - emotion_output_acc: 0.7107 - val_loss: 9.2362 - val_gender_output_loss: 0.8370 - val_image_quality_output_loss: 1.1444 - val_age_output_loss: 1.6806 - val_weight_output_loss: 1.1353 - val_bag_output_loss: 1.0787 - val_footwear_output_loss: 1.2722 - val_pose_output_loss: 1.0745 - val_emotion_output_loss: 1.0135 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4017 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4420 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "\n",
            "Epoch 00010: saving model to /content/gdrive/My Drive/person_attributes_densenet_augment.hdf5\n",
            "Epoch 11/100\n",
            "360/360 [==============================] - 150s 416ms/step - loss: 7.8742 - gender_output_loss: 0.6858 - image_quality_output_loss: 0.9814 - age_output_loss: 1.4287 - weight_output_loss: 0.9816 - bag_output_loss: 0.9175 - footwear_output_loss: 1.0419 - pose_output_loss: 0.9291 - emotion_output_loss: 0.9083 - gender_output_acc: 0.5613 - image_quality_output_acc: 0.5546 - age_output_acc: 0.3974 - weight_output_acc: 0.6350 - bag_output_acc: 0.5657 - footwear_output_acc: 0.4447 - pose_output_acc: 0.6168 - emotion_output_acc: 0.7109 - val_loss: 8.2370 - val_gender_output_loss: 0.7271 - val_image_quality_output_loss: 1.0247 - val_age_output_loss: 1.4954 - val_weight_output_loss: 1.0200 - val_bag_output_loss: 0.9771 - val_footwear_output_loss: 1.1266 - val_pose_output_loss: 0.9386 - val_emotion_output_loss: 0.9275 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4037 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4420 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "Epoch 12/100\n",
            "360/360 [==============================] - 151s 419ms/step - loss: 7.8717 - gender_output_loss: 0.6862 - image_quality_output_loss: 0.9812 - age_output_loss: 1.4289 - weight_output_loss: 0.9813 - bag_output_loss: 0.9163 - footwear_output_loss: 1.0411 - pose_output_loss: 0.9281 - emotion_output_loss: 0.9087 - gender_output_acc: 0.5609 - image_quality_output_acc: 0.5543 - age_output_acc: 0.3977 - weight_output_acc: 0.6352 - bag_output_acc: 0.5660 - footwear_output_acc: 0.4450 - pose_output_acc: 0.6167 - emotion_output_acc: 0.7108 - val_loss: 8.1821 - val_gender_output_loss: 0.6961 - val_image_quality_output_loss: 1.0348 - val_age_output_loss: 1.4956 - val_weight_output_loss: 1.0335 - val_bag_output_loss: 0.9540 - val_footwear_output_loss: 1.1052 - val_pose_output_loss: 0.9308 - val_emotion_output_loss: 0.9321 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4042 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "Epoch 13/100\n",
            "360/360 [==============================] - 151s 418ms/step - loss: 7.8700 - gender_output_loss: 0.6858 - image_quality_output_loss: 0.9811 - age_output_loss: 1.4286 - weight_output_loss: 0.9809 - bag_output_loss: 0.9164 - footwear_output_loss: 1.0409 - pose_output_loss: 0.9277 - emotion_output_loss: 0.9086 - gender_output_acc: 0.5613 - image_quality_output_acc: 0.5541 - age_output_acc: 0.3975 - weight_output_acc: 0.6356 - bag_output_acc: 0.5658 - footwear_output_acc: 0.4451 - pose_output_acc: 0.6168 - emotion_output_acc: 0.7109 - val_loss: 8.3708 - val_gender_output_loss: 0.7340 - val_image_quality_output_loss: 1.0575 - val_age_output_loss: 1.5299 - val_weight_output_loss: 1.0453 - val_bag_output_loss: 0.9844 - val_footwear_output_loss: 1.1140 - val_pose_output_loss: 0.9669 - val_emotion_output_loss: 0.9387 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4027 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4430 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "Epoch 14/100\n",
            "360/360 [==============================] - 150s 418ms/step - loss: 7.8697 - gender_output_loss: 0.6855 - image_quality_output_loss: 0.9812 - age_output_loss: 1.4288 - weight_output_loss: 0.9814 - bag_output_loss: 0.9163 - footwear_output_loss: 1.0410 - pose_output_loss: 0.9283 - emotion_output_loss: 0.9071 - gender_output_acc: 0.5615 - image_quality_output_acc: 0.5543 - age_output_acc: 0.3974 - weight_output_acc: 0.6352 - bag_output_acc: 0.5659 - footwear_output_acc: 0.4448 - pose_output_acc: 0.6163 - emotion_output_acc: 0.7113 - val_loss: 8.4176 - val_gender_output_loss: 0.7267 - val_image_quality_output_loss: 1.0526 - val_age_output_loss: 1.5536 - val_weight_output_loss: 1.0464 - val_bag_output_loss: 0.9909 - val_footwear_output_loss: 1.1269 - val_pose_output_loss: 0.9809 - val_emotion_output_loss: 0.9396 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4017 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4430 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "Epoch 15/100\n",
            "360/360 [==============================] - 151s 419ms/step - loss: 7.8694 - gender_output_loss: 0.6856 - image_quality_output_loss: 0.9802 - age_output_loss: 1.4281 - weight_output_loss: 0.9813 - bag_output_loss: 0.9172 - footwear_output_loss: 1.0412 - pose_output_loss: 0.9277 - emotion_output_loss: 0.9080 - gender_output_acc: 0.5612 - image_quality_output_acc: 0.5543 - age_output_acc: 0.3981 - weight_output_acc: 0.6353 - bag_output_acc: 0.5657 - footwear_output_acc: 0.4448 - pose_output_acc: 0.6168 - emotion_output_acc: 0.7110 - val_loss: 8.2403 - val_gender_output_loss: 0.6947 - val_image_quality_output_loss: 1.0339 - val_age_output_loss: 1.5276 - val_weight_output_loss: 1.0289 - val_bag_output_loss: 0.9578 - val_footwear_output_loss: 1.1190 - val_pose_output_loss: 0.9439 - val_emotion_output_loss: 0.9345 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4017 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4420 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "Epoch 16/100\n",
            "360/360 [==============================] - 151s 419ms/step - loss: 7.8696 - gender_output_loss: 0.6858 - image_quality_output_loss: 0.9806 - age_output_loss: 1.4288 - weight_output_loss: 0.9817 - bag_output_loss: 0.9164 - footwear_output_loss: 1.0410 - pose_output_loss: 0.9274 - emotion_output_loss: 0.9080 - gender_output_acc: 0.5611 - image_quality_output_acc: 0.5544 - age_output_acc: 0.3978 - weight_output_acc: 0.6354 - bag_output_acc: 0.5658 - footwear_output_acc: 0.4446 - pose_output_acc: 0.6168 - emotion_output_acc: 0.7108 - val_loss: 8.4158 - val_gender_output_loss: 0.7254 - val_image_quality_output_loss: 1.0608 - val_age_output_loss: 1.5420 - val_weight_output_loss: 1.0544 - val_bag_output_loss: 0.9912 - val_footwear_output_loss: 1.1271 - val_pose_output_loss: 0.9747 - val_emotion_output_loss: 0.9402 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4027 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4430 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "Epoch 17/100\n",
            "360/360 [==============================] - 151s 420ms/step - loss: 7.8718 - gender_output_loss: 0.6859 - image_quality_output_loss: 0.9814 - age_output_loss: 1.4286 - weight_output_loss: 0.9813 - bag_output_loss: 0.9163 - footwear_output_loss: 1.0412 - pose_output_loss: 0.9287 - emotion_output_loss: 0.9084 - gender_output_acc: 0.5610 - image_quality_output_acc: 0.5541 - age_output_acc: 0.3975 - weight_output_acc: 0.6354 - bag_output_acc: 0.5661 - footwear_output_acc: 0.4446 - pose_output_acc: 0.6165 - emotion_output_acc: 0.7109 - val_loss: 8.2579 - val_gender_output_loss: 0.7108 - val_image_quality_output_loss: 1.0434 - val_age_output_loss: 1.5163 - val_weight_output_loss: 1.0392 - val_bag_output_loss: 0.9609 - val_footwear_output_loss: 1.1067 - val_pose_output_loss: 0.9450 - val_emotion_output_loss: 0.9356 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4032 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4435 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "Epoch 18/100\n",
            "360/360 [==============================] - 152s 422ms/step - loss: 7.8699 - gender_output_loss: 0.6857 - image_quality_output_loss: 0.9809 - age_output_loss: 1.4286 - weight_output_loss: 0.9817 - bag_output_loss: 0.9163 - footwear_output_loss: 1.0408 - pose_output_loss: 0.9280 - emotion_output_loss: 0.9078 - gender_output_acc: 0.5611 - image_quality_output_acc: 0.5543 - age_output_acc: 0.3979 - weight_output_acc: 0.6350 - bag_output_acc: 0.5661 - footwear_output_acc: 0.4448 - pose_output_acc: 0.6168 - emotion_output_acc: 0.7109 - val_loss: 8.3667 - val_gender_output_loss: 0.7136 - val_image_quality_output_loss: 1.0467 - val_age_output_loss: 1.5515 - val_weight_output_loss: 1.0391 - val_bag_output_loss: 0.9776 - val_footwear_output_loss: 1.1307 - val_pose_output_loss: 0.9716 - val_emotion_output_loss: 0.9359 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4022 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4430 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "Epoch 19/100\n",
            "360/360 [==============================] - 152s 422ms/step - loss: 7.8707 - gender_output_loss: 0.6858 - image_quality_output_loss: 0.9810 - age_output_loss: 1.4284 - weight_output_loss: 0.9816 - bag_output_loss: 0.9160 - footwear_output_loss: 1.0419 - pose_output_loss: 0.9282 - emotion_output_loss: 0.9077 - gender_output_acc: 0.5614 - image_quality_output_acc: 0.5543 - age_output_acc: 0.3976 - weight_output_acc: 0.6352 - bag_output_acc: 0.5661 - footwear_output_acc: 0.4444 - pose_output_acc: 0.6167 - emotion_output_acc: 0.7109 - val_loss: 8.0844 - val_gender_output_loss: 0.6965 - val_image_quality_output_loss: 1.0159 - val_age_output_loss: 1.4842 - val_weight_output_loss: 1.0089 - val_bag_output_loss: 0.9460 - val_footwear_output_loss: 1.0861 - val_pose_output_loss: 0.9318 - val_emotion_output_loss: 0.9150 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4027 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "Epoch 20/100\n",
            "360/360 [==============================] - 152s 423ms/step - loss: 7.8705 - gender_output_loss: 0.6859 - image_quality_output_loss: 0.9807 - age_output_loss: 1.4280 - weight_output_loss: 0.9825 - bag_output_loss: 0.9162 - footwear_output_loss: 1.0409 - pose_output_loss: 0.9281 - emotion_output_loss: 0.9083 - gender_output_acc: 0.5613 - image_quality_output_acc: 0.5545 - age_output_acc: 0.3977 - weight_output_acc: 0.6351 - bag_output_acc: 0.5659 - footwear_output_acc: 0.4450 - pose_output_acc: 0.6168 - emotion_output_acc: 0.7109 - val_loss: 8.3018 - val_gender_output_loss: 0.6938 - val_image_quality_output_loss: 1.0335 - val_age_output_loss: 1.5424 - val_weight_output_loss: 1.0332 - val_bag_output_loss: 0.9628 - val_footwear_output_loss: 1.1387 - val_pose_output_loss: 0.9614 - val_emotion_output_loss: 0.9359 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4022 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4410 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "\n",
            "Epoch 00020: saving model to /content/gdrive/My Drive/person_attributes_densenet_augment.hdf5\n",
            "Epoch 21/100\n",
            "360/360 [==============================] - 151s 419ms/step - loss: 7.8714 - gender_output_loss: 0.6857 - image_quality_output_loss: 0.9810 - age_output_loss: 1.4285 - weight_output_loss: 0.9818 - bag_output_loss: 0.9167 - footwear_output_loss: 1.0413 - pose_output_loss: 0.9280 - emotion_output_loss: 0.9084 - gender_output_acc: 0.5613 - image_quality_output_acc: 0.5543 - age_output_acc: 0.3977 - weight_output_acc: 0.6352 - bag_output_acc: 0.5661 - footwear_output_acc: 0.4448 - pose_output_acc: 0.6168 - emotion_output_acc: 0.7110 - val_loss: 8.2351 - val_gender_output_loss: 0.7015 - val_image_quality_output_loss: 1.0389 - val_age_output_loss: 1.5227 - val_weight_output_loss: 1.0322 - val_bag_output_loss: 0.9478 - val_footwear_output_loss: 1.1149 - val_pose_output_loss: 0.9486 - val_emotion_output_loss: 0.9285 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4022 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "Epoch 22/100\n",
            "360/360 [==============================] - 152s 421ms/step - loss: 7.8688 - gender_output_loss: 0.6858 - image_quality_output_loss: 0.9806 - age_output_loss: 1.4280 - weight_output_loss: 0.9817 - bag_output_loss: 0.9161 - footwear_output_loss: 1.0408 - pose_output_loss: 0.9276 - emotion_output_loss: 0.9082 - gender_output_acc: 0.5609 - image_quality_output_acc: 0.5546 - age_output_acc: 0.3979 - weight_output_acc: 0.6352 - bag_output_acc: 0.5657 - footwear_output_acc: 0.4446 - pose_output_acc: 0.6168 - emotion_output_acc: 0.7106 - val_loss: 8.2915 - val_gender_output_loss: 0.7191 - val_image_quality_output_loss: 1.0467 - val_age_output_loss: 1.5146 - val_weight_output_loss: 1.0392 - val_bag_output_loss: 0.9710 - val_footwear_output_loss: 1.1110 - val_pose_output_loss: 0.9611 - val_emotion_output_loss: 0.9288 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4032 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "Epoch 23/100\n",
            "360/360 [==============================] - 150s 418ms/step - loss: 7.8689 - gender_output_loss: 0.6858 - image_quality_output_loss: 0.9807 - age_output_loss: 1.4283 - weight_output_loss: 0.9811 - bag_output_loss: 0.9158 - footwear_output_loss: 1.0408 - pose_output_loss: 0.9283 - emotion_output_loss: 0.9081 - gender_output_acc: 0.5614 - image_quality_output_acc: 0.5543 - age_output_acc: 0.3978 - weight_output_acc: 0.6354 - bag_output_acc: 0.5659 - footwear_output_acc: 0.4449 - pose_output_acc: 0.6166 - emotion_output_acc: 0.7111 - val_loss: 8.2418 - val_gender_output_loss: 0.7025 - val_image_quality_output_loss: 1.0393 - val_age_output_loss: 1.5278 - val_weight_output_loss: 1.0327 - val_bag_output_loss: 0.9481 - val_footwear_output_loss: 1.1167 - val_pose_output_loss: 0.9471 - val_emotion_output_loss: 0.9276 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4027 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4430 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "Epoch 24/100\n",
            "360/360 [==============================] - 151s 418ms/step - loss: 7.8687 - gender_output_loss: 0.6859 - image_quality_output_loss: 0.9810 - age_output_loss: 1.4287 - weight_output_loss: 0.9812 - bag_output_loss: 0.9162 - footwear_output_loss: 1.0404 - pose_output_loss: 0.9278 - emotion_output_loss: 0.9074 - gender_output_acc: 0.5613 - image_quality_output_acc: 0.5543 - age_output_acc: 0.3976 - weight_output_acc: 0.6355 - bag_output_acc: 0.5655 - footwear_output_acc: 0.4450 - pose_output_acc: 0.6166 - emotion_output_acc: 0.7109 - val_loss: 8.2222 - val_gender_output_loss: 0.7033 - val_image_quality_output_loss: 1.0404 - val_age_output_loss: 1.5231 - val_weight_output_loss: 1.0344 - val_bag_output_loss: 0.9458 - val_footwear_output_loss: 1.1096 - val_pose_output_loss: 0.9378 - val_emotion_output_loss: 0.9278 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4027 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "Epoch 25/100\n",
            "360/360 [==============================] - 151s 419ms/step - loss: 7.8702 - gender_output_loss: 0.6858 - image_quality_output_loss: 0.9811 - age_output_loss: 1.4285 - weight_output_loss: 0.9817 - bag_output_loss: 0.9160 - footwear_output_loss: 1.0410 - pose_output_loss: 0.9280 - emotion_output_loss: 0.9081 - gender_output_acc: 0.5609 - image_quality_output_acc: 0.5543 - age_output_acc: 0.3978 - weight_output_acc: 0.6352 - bag_output_acc: 0.5657 - footwear_output_acc: 0.4448 - pose_output_acc: 0.6164 - emotion_output_acc: 0.7109 - val_loss: 8.3633 - val_gender_output_loss: 0.7177 - val_image_quality_output_loss: 1.0532 - val_age_output_loss: 1.5339 - val_weight_output_loss: 1.0490 - val_bag_output_loss: 0.9799 - val_footwear_output_loss: 1.1271 - val_pose_output_loss: 0.9698 - val_emotion_output_loss: 0.9327 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4032 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4430 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "Epoch 26/100\n",
            "360/360 [==============================] - 151s 420ms/step - loss: 7.8702 - gender_output_loss: 0.6857 - image_quality_output_loss: 0.9807 - age_output_loss: 1.4284 - weight_output_loss: 0.9812 - bag_output_loss: 0.9166 - footwear_output_loss: 1.0413 - pose_output_loss: 0.9282 - emotion_output_loss: 0.9081 - gender_output_acc: 0.5615 - image_quality_output_acc: 0.5542 - age_output_acc: 0.3979 - weight_output_acc: 0.6355 - bag_output_acc: 0.5659 - footwear_output_acc: 0.4448 - pose_output_acc: 0.6167 - emotion_output_acc: 0.7107 - val_loss: 8.2935 - val_gender_output_loss: 0.7021 - val_image_quality_output_loss: 1.0383 - val_age_output_loss: 1.5399 - val_weight_output_loss: 1.0313 - val_bag_output_loss: 0.9593 - val_footwear_output_loss: 1.1276 - val_pose_output_loss: 0.9606 - val_emotion_output_loss: 0.9344 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4022 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4425 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "Epoch 27/100\n",
            "360/360 [==============================] - 151s 421ms/step - loss: 7.8767 - gender_output_loss: 0.6878 - image_quality_output_loss: 0.9812 - age_output_loss: 1.4298 - weight_output_loss: 0.9818 - bag_output_loss: 0.9173 - footwear_output_loss: 1.0409 - pose_output_loss: 0.9289 - emotion_output_loss: 0.9090 - gender_output_acc: 0.5612 - image_quality_output_acc: 0.5543 - age_output_acc: 0.3977 - weight_output_acc: 0.6352 - bag_output_acc: 0.5660 - footwear_output_acc: 0.4448 - pose_output_acc: 0.6168 - emotion_output_acc: 0.7108 - val_loss: 8.4654 - val_gender_output_loss: 0.7731 - val_image_quality_output_loss: 1.0861 - val_age_output_loss: 1.5333 - val_weight_output_loss: 1.0628 - val_bag_output_loss: 0.9956 - val_footwear_output_loss: 1.1254 - val_pose_output_loss: 0.9518 - val_emotion_output_loss: 0.9373 - val_gender_output_acc: 0.5696 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4027 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4486 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "Epoch 28/100\n",
            "360/360 [==============================] - 152s 421ms/step - loss: 7.8696 - gender_output_loss: 0.6858 - image_quality_output_loss: 0.9799 - age_output_loss: 1.4287 - weight_output_loss: 0.9817 - bag_output_loss: 0.9163 - footwear_output_loss: 1.0414 - pose_output_loss: 0.9275 - emotion_output_loss: 0.9083 - gender_output_acc: 0.5609 - image_quality_output_acc: 0.5545 - age_output_acc: 0.3978 - weight_output_acc: 0.6352 - bag_output_acc: 0.5656 - footwear_output_acc: 0.4446 - pose_output_acc: 0.6168 - emotion_output_acc: 0.7108 - val_loss: 7.8655 - val_gender_output_loss: 0.6834 - val_image_quality_output_loss: 0.9906 - val_age_output_loss: 1.4273 - val_weight_output_loss: 0.9868 - val_bag_output_loss: 0.9280 - val_footwear_output_loss: 1.0381 - val_pose_output_loss: 0.9186 - val_emotion_output_loss: 0.8927 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4052 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4456 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "Epoch 29/100\n",
            "360/360 [==============================] - 152s 423ms/step - loss: 7.8700 - gender_output_loss: 0.6858 - image_quality_output_loss: 0.9808 - age_output_loss: 1.4285 - weight_output_loss: 0.9821 - bag_output_loss: 0.9159 - footwear_output_loss: 1.0411 - pose_output_loss: 0.9275 - emotion_output_loss: 0.9083 - gender_output_acc: 0.5612 - image_quality_output_acc: 0.5545 - age_output_acc: 0.3977 - weight_output_acc: 0.6350 - bag_output_acc: 0.5657 - footwear_output_acc: 0.4448 - pose_output_acc: 0.6168 - emotion_output_acc: 0.7109 - val_loss: 7.8671 - val_gender_output_loss: 0.6838 - val_image_quality_output_loss: 0.9902 - val_age_output_loss: 1.4268 - val_weight_output_loss: 0.9863 - val_bag_output_loss: 0.9271 - val_footwear_output_loss: 1.0385 - val_pose_output_loss: 0.9196 - val_emotion_output_loss: 0.8948 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4052 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4456 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "Epoch 30/100\n",
            "360/360 [==============================] - 153s 424ms/step - loss: 7.8696 - gender_output_loss: 0.6858 - image_quality_output_loss: 0.9808 - age_output_loss: 1.4287 - weight_output_loss: 0.9816 - bag_output_loss: 0.9159 - footwear_output_loss: 1.0411 - pose_output_loss: 0.9274 - emotion_output_loss: 0.9083 - gender_output_acc: 0.5611 - image_quality_output_acc: 0.5542 - age_output_acc: 0.3975 - weight_output_acc: 0.6352 - bag_output_acc: 0.5661 - footwear_output_acc: 0.4447 - pose_output_acc: 0.6169 - emotion_output_acc: 0.7107 - val_loss: 7.8629 - val_gender_output_loss: 0.6836 - val_image_quality_output_loss: 0.9903 - val_age_output_loss: 1.4265 - val_weight_output_loss: 0.9862 - val_bag_output_loss: 0.9259 - val_footwear_output_loss: 1.0381 - val_pose_output_loss: 0.9188 - val_emotion_output_loss: 0.8933 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4052 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4456 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "\n",
            "Epoch 00030: saving model to /content/gdrive/My Drive/person_attributes_densenet_augment.hdf5\n",
            "Epoch 31/100\n",
            "360/360 [==============================] - 152s 422ms/step - loss: 7.8697 - gender_output_loss: 0.6857 - image_quality_output_loss: 0.9809 - age_output_loss: 1.4282 - weight_output_loss: 0.9820 - bag_output_loss: 0.9156 - footwear_output_loss: 1.0409 - pose_output_loss: 0.9280 - emotion_output_loss: 0.9083 - gender_output_acc: 0.5611 - image_quality_output_acc: 0.5541 - age_output_acc: 0.3977 - weight_output_acc: 0.6351 - bag_output_acc: 0.5660 - footwear_output_acc: 0.4448 - pose_output_acc: 0.6167 - emotion_output_acc: 0.7106 - val_loss: 7.8669 - val_gender_output_loss: 0.6833 - val_image_quality_output_loss: 0.9904 - val_age_output_loss: 1.4272 - val_weight_output_loss: 0.9861 - val_bag_output_loss: 0.9285 - val_footwear_output_loss: 1.0382 - val_pose_output_loss: 0.9186 - val_emotion_output_loss: 0.8947 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4052 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4456 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "Epoch 32/100\n",
            "360/360 [==============================] - 151s 418ms/step - loss: 7.8686 - gender_output_loss: 0.6857 - image_quality_output_loss: 0.9805 - age_output_loss: 1.4284 - weight_output_loss: 0.9811 - bag_output_loss: 0.9160 - footwear_output_loss: 1.0410 - pose_output_loss: 0.9277 - emotion_output_loss: 0.9082 - gender_output_acc: 0.5614 - image_quality_output_acc: 0.5544 - age_output_acc: 0.3978 - weight_output_acc: 0.6354 - bag_output_acc: 0.5660 - footwear_output_acc: 0.4444 - pose_output_acc: 0.6166 - emotion_output_acc: 0.7108 - val_loss: 7.8669 - val_gender_output_loss: 0.6835 - val_image_quality_output_loss: 0.9907 - val_age_output_loss: 1.4264 - val_weight_output_loss: 0.9873 - val_bag_output_loss: 0.9273 - val_footwear_output_loss: 1.0383 - val_pose_output_loss: 0.9187 - val_emotion_output_loss: 0.8948 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4052 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4456 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "Epoch 33/100\n",
            "359/360 [============================>.] - ETA: 0s - loss: 7.8687 - gender_output_loss: 0.6857 - image_quality_output_loss: 0.9810 - age_output_loss: 1.4284 - weight_output_loss: 0.9821 - bag_output_loss: 0.9156 - footwear_output_loss: 1.0405 - pose_output_loss: 0.9272 - emotion_output_loss: 0.9082 - gender_output_acc: 0.5614 - image_quality_output_acc: 0.5540 - age_output_acc: 0.3979 - weight_output_acc: 0.6349 - bag_output_acc: 0.5666 - footwear_output_acc: 0.4444 - pose_output_acc: 0.6169 - emotion_output_acc: 0.7107Epoch 33/100\n",
            "360/360 [==============================] - 151s 420ms/step - loss: 7.8684 - gender_output_loss: 0.6857 - image_quality_output_loss: 0.9808 - age_output_loss: 1.4285 - weight_output_loss: 0.9817 - bag_output_loss: 0.9158 - footwear_output_loss: 1.0405 - pose_output_loss: 0.9273 - emotion_output_loss: 0.9081 - gender_output_acc: 0.5615 - image_quality_output_acc: 0.5541 - age_output_acc: 0.3977 - weight_output_acc: 0.6351 - bag_output_acc: 0.5661 - footwear_output_acc: 0.4444 - pose_output_acc: 0.6168 - emotion_output_acc: 0.7108 - val_loss: 7.8653 - val_gender_output_loss: 0.6833 - val_image_quality_output_loss: 0.9908 - val_age_output_loss: 1.4260 - val_weight_output_loss: 0.9866 - val_bag_output_loss: 0.9262 - val_footwear_output_loss: 1.0385 - val_pose_output_loss: 0.9195 - val_emotion_output_loss: 0.8944 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4052 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4456 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "Epoch 34/100\n",
            "360/360 [==============================] - 151s 420ms/step - loss: 7.8676 - gender_output_loss: 0.6857 - image_quality_output_loss: 0.9802 - age_output_loss: 1.4288 - weight_output_loss: 0.9807 - bag_output_loss: 0.9167 - footwear_output_loss: 1.0407 - pose_output_loss: 0.9270 - emotion_output_loss: 0.9079 - gender_output_acc: 0.5614 - image_quality_output_acc: 0.5544 - age_output_acc: 0.3974 - weight_output_acc: 0.6355 - bag_output_acc: 0.5661 - footwear_output_acc: 0.4451 - pose_output_acc: 0.6167 - emotion_output_acc: 0.7109 - val_loss: 7.8673 - val_gender_output_loss: 0.6833 - val_image_quality_output_loss: 0.9905 - val_age_output_loss: 1.4259 - val_weight_output_loss: 0.9862 - val_bag_output_loss: 0.9278 - val_footwear_output_loss: 1.0384 - val_pose_output_loss: 0.9189 - val_emotion_output_loss: 0.8963 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4052 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4456 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "Epoch 35/100\n",
            "360/360 [==============================] - 151s 420ms/step - loss: 7.8691 - gender_output_loss: 0.6857 - image_quality_output_loss: 0.9805 - age_output_loss: 1.4283 - weight_output_loss: 0.9817 - bag_output_loss: 0.9159 - footwear_output_loss: 1.0408 - pose_output_loss: 0.9282 - emotion_output_loss: 0.9080 - gender_output_acc: 0.5615 - image_quality_output_acc: 0.5546 - age_output_acc: 0.3979 - weight_output_acc: 0.6351 - bag_output_acc: 0.5657 - footwear_output_acc: 0.4451 - pose_output_acc: 0.6168 - emotion_output_acc: 0.7109 - val_loss: 7.8656 - val_gender_output_loss: 0.6833 - val_image_quality_output_loss: 0.9905 - val_age_output_loss: 1.4255 - val_weight_output_loss: 0.9886 - val_bag_output_loss: 0.9263 - val_footwear_output_loss: 1.0382 - val_pose_output_loss: 0.9187 - val_emotion_output_loss: 0.8944 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4052 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4456 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "Epoch 36/100\n",
            "360/360 [==============================] - 151s 421ms/step - loss: 7.8695 - gender_output_loss: 0.6858 - image_quality_output_loss: 0.9810 - age_output_loss: 1.4286 - weight_output_loss: 0.9819 - bag_output_loss: 0.9159 - footwear_output_loss: 1.0407 - pose_output_loss: 0.9284 - emotion_output_loss: 0.9072 - gender_output_acc: 0.5611 - image_quality_output_acc: 0.5540 - age_output_acc: 0.3975 - weight_output_acc: 0.6350 - bag_output_acc: 0.5656 - footwear_output_acc: 0.4451 - pose_output_acc: 0.6168 - emotion_output_acc: 0.7111 - val_loss: 7.8661 - val_gender_output_loss: 0.6832 - val_image_quality_output_loss: 0.9902 - val_age_output_loss: 1.4255 - val_weight_output_loss: 0.9863 - val_bag_output_loss: 0.9280 - val_footwear_output_loss: 1.0382 - val_pose_output_loss: 0.9194 - val_emotion_output_loss: 0.8953 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4052 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4456 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "Epoch 37/100\n",
            "360/360 [==============================] - 151s 420ms/step - loss: 7.8684 - gender_output_loss: 0.6856 - image_quality_output_loss: 0.9802 - age_output_loss: 1.4285 - weight_output_loss: 0.9810 - bag_output_loss: 0.9166 - footwear_output_loss: 1.0409 - pose_output_loss: 0.9277 - emotion_output_loss: 0.9079 - gender_output_acc: 0.5613 - image_quality_output_acc: 0.5543 - age_output_acc: 0.3975 - weight_output_acc: 0.6355 - bag_output_acc: 0.5658 - footwear_output_acc: 0.4449 - pose_output_acc: 0.6169 - emotion_output_acc: 0.7106 - val_loss: 7.8630 - val_gender_output_loss: 0.6835 - val_image_quality_output_loss: 0.9902 - val_age_output_loss: 1.4263 - val_weight_output_loss: 0.9868 - val_bag_output_loss: 0.9259 - val_footwear_output_loss: 1.0384 - val_pose_output_loss: 0.9184 - val_emotion_output_loss: 0.8935 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4052 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4456 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "Epoch 38/100\n",
            "360/360 [==============================] - 152s 421ms/step - loss: 7.8670 - gender_output_loss: 0.6857 - image_quality_output_loss: 0.9802 - age_output_loss: 1.4285 - weight_output_loss: 0.9812 - bag_output_loss: 0.9157 - footwear_output_loss: 1.0407 - pose_output_loss: 0.9275 - emotion_output_loss: 0.9075 - gender_output_acc: 0.5612 - image_quality_output_acc: 0.5541 - age_output_acc: 0.3976 - weight_output_acc: 0.6353 - bag_output_acc: 0.5659 - footwear_output_acc: 0.4446 - pose_output_acc: 0.6168 - emotion_output_acc: 0.7109 - val_loss: 7.8633 - val_gender_output_loss: 0.6834 - val_image_quality_output_loss: 0.9905 - val_age_output_loss: 1.4250 - val_weight_output_loss: 0.9854 - val_bag_output_loss: 0.9275 - val_footwear_output_loss: 1.0382 - val_pose_output_loss: 0.9183 - val_emotion_output_loss: 0.8949 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4052 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4456 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "Epoch 39/100\n",
            "360/360 [==============================] - 151s 419ms/step - loss: 7.8698 - gender_output_loss: 0.6858 - image_quality_output_loss: 0.9812 - age_output_loss: 1.4286 - weight_output_loss: 0.9814 - bag_output_loss: 0.9158 - footwear_output_loss: 1.0408 - pose_output_loss: 0.9282 - emotion_output_loss: 0.9080 - gender_output_acc: 0.5609 - image_quality_output_acc: 0.5543 - age_output_acc: 0.3974 - weight_output_acc: 0.6352 - bag_output_acc: 0.5659 - footwear_output_acc: 0.4448 - pose_output_acc: 0.6163 - emotion_output_acc: 0.7107 - val_loss: 7.8653 - val_gender_output_loss: 0.6833 - val_image_quality_output_loss: 0.9906 - val_age_output_loss: 1.4259 - val_weight_output_loss: 0.9875 - val_bag_output_loss: 0.9266 - val_footwear_output_loss: 1.0385 - val_pose_output_loss: 0.9183 - val_emotion_output_loss: 0.8948 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4052 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4456 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "Epoch 40/100\n",
            "360/360 [==============================] - 151s 420ms/step - loss: 7.8677 - gender_output_loss: 0.6857 - image_quality_output_loss: 0.9805 - age_output_loss: 1.4279 - weight_output_loss: 0.9816 - bag_output_loss: 0.9155 - footwear_output_loss: 1.0410 - pose_output_loss: 0.9281 - emotion_output_loss: 0.9075 - gender_output_acc: 0.5612 - image_quality_output_acc: 0.5544 - age_output_acc: 0.3979 - weight_output_acc: 0.6351 - bag_output_acc: 0.5659 - footwear_output_acc: 0.4448 - pose_output_acc: 0.6168 - emotion_output_acc: 0.7111 - val_loss: 7.8653 - val_gender_output_loss: 0.6832 - val_image_quality_output_loss: 0.9905 - val_age_output_loss: 1.4258 - val_weight_output_loss: 0.9866 - val_bag_output_loss: 0.9270 - val_footwear_output_loss: 1.0383 - val_pose_output_loss: 0.9187 - val_emotion_output_loss: 0.8951 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4052 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4456 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "\n",
            "Epoch 00040: saving model to /content/gdrive/My Drive/person_attributes_densenet_augment.hdf5\n",
            "Epoch 41/100\n",
            "360/360 [==============================] - 151s 418ms/step - loss: 7.8676 - gender_output_loss: 0.6858 - image_quality_output_loss: 0.9810 - age_output_loss: 1.4282 - weight_output_loss: 0.9809 - bag_output_loss: 0.9163 - footwear_output_loss: 1.0407 - pose_output_loss: 0.9275 - emotion_output_loss: 0.9072 - gender_output_acc: 0.5611 - image_quality_output_acc: 0.5541 - age_output_acc: 0.3979 - weight_output_acc: 0.6354 - bag_output_acc: 0.5656 - footwear_output_acc: 0.4448 - pose_output_acc: 0.6167 - emotion_output_acc: 0.7111 - val_loss: 7.8639 - val_gender_output_loss: 0.6833 - val_image_quality_output_loss: 0.9907 - val_age_output_loss: 1.4258 - val_weight_output_loss: 0.9860 - val_bag_output_loss: 0.9260 - val_footwear_output_loss: 1.0382 - val_pose_output_loss: 0.9183 - val_emotion_output_loss: 0.8955 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4052 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4456 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "Epoch 42/100\n",
            "360/360 [==============================] - 151s 419ms/step - loss: 7.8673 - gender_output_loss: 0.6858 - image_quality_output_loss: 0.9805 - age_output_loss: 1.4284 - weight_output_loss: 0.9811 - bag_output_loss: 0.9160 - footwear_output_loss: 1.0407 - pose_output_loss: 0.9273 - emotion_output_loss: 0.9076 - gender_output_acc: 0.5610 - image_quality_output_acc: 0.5542 - age_output_acc: 0.3976 - weight_output_acc: 0.6352 - bag_output_acc: 0.5657 - footwear_output_acc: 0.4450 - pose_output_acc: 0.6168 - emotion_output_acc: 0.7110 - val_loss: 7.8660 - val_gender_output_loss: 0.6833 - val_image_quality_output_loss: 0.9906 - val_age_output_loss: 1.4270 - val_weight_output_loss: 0.9874 - val_bag_output_loss: 0.9279 - val_footwear_output_loss: 1.0382 - val_pose_output_loss: 0.9184 - val_emotion_output_loss: 0.8933 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4052 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4456 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "Epoch 43/100\n",
            "360/360 [==============================] - 151s 419ms/step - loss: 7.8671 - gender_output_loss: 0.6857 - image_quality_output_loss: 0.9804 - age_output_loss: 1.4285 - weight_output_loss: 0.9809 - bag_output_loss: 0.9158 - footwear_output_loss: 1.0406 - pose_output_loss: 0.9279 - emotion_output_loss: 0.9073 - gender_output_acc: 0.5612 - image_quality_output_acc: 0.5543 - age_output_acc: 0.3977 - weight_output_acc: 0.6353 - bag_output_acc: 0.5659 - footwear_output_acc: 0.4446 - pose_output_acc: 0.6167 - emotion_output_acc: 0.7109 - val_loss: 7.8617 - val_gender_output_loss: 0.6832 - val_image_quality_output_loss: 0.9904 - val_age_output_loss: 1.4255 - val_weight_output_loss: 0.9852 - val_bag_output_loss: 0.9265 - val_footwear_output_loss: 1.0381 - val_pose_output_loss: 0.9187 - val_emotion_output_loss: 0.8940 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4052 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4456 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "Epoch 44/100\n",
            "360/360 [==============================] - 151s 419ms/step - loss: 7.8669 - gender_output_loss: 0.6857 - image_quality_output_loss: 0.9802 - age_output_loss: 1.4285 - weight_output_loss: 0.9808 - bag_output_loss: 0.9155 - footwear_output_loss: 1.0408 - pose_output_loss: 0.9276 - emotion_output_loss: 0.9079 - gender_output_acc: 0.5613 - image_quality_output_acc: 0.5545 - age_output_acc: 0.3975 - weight_output_acc: 0.6353 - bag_output_acc: 0.5663 - footwear_output_acc: 0.4448 - pose_output_acc: 0.6166 - emotion_output_acc: 0.7108 - val_loss: 7.8632 - val_gender_output_loss: 0.6834 - val_image_quality_output_loss: 0.9909 - val_age_output_loss: 1.4246 - val_weight_output_loss: 0.9863 - val_bag_output_loss: 0.9263 - val_footwear_output_loss: 1.0381 - val_pose_output_loss: 0.9185 - val_emotion_output_loss: 0.8951 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4052 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4456 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "Epoch 45/100\n",
            "359/360 [============================>.] - ETA: 0s - loss: 7.8678 - gender_output_loss: 0.6857 - image_quality_output_loss: 0.9803 - age_output_loss: 1.4282 - weight_output_loss: 0.9814 - bag_output_loss: 0.9165 - footwear_output_loss: 1.0411 - pose_output_loss: 0.9272 - emotion_output_loss: 0.9076 - gender_output_acc: 0.5613 - image_quality_output_acc: 0.5546 - age_output_acc: 0.3977 - weight_output_acc: 0.6348 - bag_output_acc: 0.5655 - footwear_output_acc: 0.4448 - pose_output_acc: 0.6166 - emotion_output_acc: 0.7110Epoch 45/100\n",
            "360/360 [==============================] - 151s 419ms/step - loss: 7.8675 - gender_output_loss: 0.6857 - image_quality_output_loss: 0.9805 - age_output_loss: 1.4285 - weight_output_loss: 0.9809 - bag_output_loss: 0.9161 - footwear_output_loss: 1.0411 - pose_output_loss: 0.9271 - emotion_output_loss: 0.9076 - gender_output_acc: 0.5612 - image_quality_output_acc: 0.5543 - age_output_acc: 0.3977 - weight_output_acc: 0.6352 - bag_output_acc: 0.5657 - footwear_output_acc: 0.4447 - pose_output_acc: 0.6168 - emotion_output_acc: 0.7109 - val_loss: 7.8666 - val_gender_output_loss: 0.6832 - val_image_quality_output_loss: 0.9913 - val_age_output_loss: 1.4260 - val_weight_output_loss: 0.9883 - val_bag_output_loss: 0.9279 - val_footwear_output_loss: 1.0382 - val_pose_output_loss: 0.9184 - val_emotion_output_loss: 0.8935 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4052 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4456 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "Epoch 46/100\n",
            "360/360 [==============================] - 151s 419ms/step - loss: 7.8675 - gender_output_loss: 0.6857 - image_quality_output_loss: 0.9805 - age_output_loss: 1.4287 - weight_output_loss: 0.9819 - bag_output_loss: 0.9155 - footwear_output_loss: 1.0404 - pose_output_loss: 0.9275 - emotion_output_loss: 0.9073 - gender_output_acc: 0.5614 - image_quality_output_acc: 0.5543 - age_output_acc: 0.3972 - weight_output_acc: 0.6349 - bag_output_acc: 0.5661 - footwear_output_acc: 0.4451 - pose_output_acc: 0.6170 - emotion_output_acc: 0.7110 - val_loss: 7.8630 - val_gender_output_loss: 0.6833 - val_image_quality_output_loss: 0.9902 - val_age_output_loss: 1.4258 - val_weight_output_loss: 0.9856 - val_bag_output_loss: 0.9271 - val_footwear_output_loss: 1.0382 - val_pose_output_loss: 0.9183 - val_emotion_output_loss: 0.8945 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4052 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4456 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "Epoch 47/100\n",
            "360/360 [==============================] - 151s 420ms/step - loss: 7.8671 - gender_output_loss: 0.6856 - image_quality_output_loss: 0.9805 - age_output_loss: 1.4279 - weight_output_loss: 0.9811 - bag_output_loss: 0.9160 - footwear_output_loss: 1.0405 - pose_output_loss: 0.9272 - emotion_output_loss: 0.9081 - gender_output_acc: 0.5615 - image_quality_output_acc: 0.5543 - age_output_acc: 0.3979 - weight_output_acc: 0.6354 - bag_output_acc: 0.5658 - footwear_output_acc: 0.4449 - pose_output_acc: 0.6168 - emotion_output_acc: 0.7107 - val_loss: 7.8651 - val_gender_output_loss: 0.6834 - val_image_quality_output_loss: 0.9903 - val_age_output_loss: 1.4267 - val_weight_output_loss: 0.9867 - val_bag_output_loss: 0.9281 - val_footwear_output_loss: 1.0387 - val_pose_output_loss: 0.9184 - val_emotion_output_loss: 0.8929 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4052 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4456 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "Epoch 48/100\n",
            "360/360 [==============================] - 151s 420ms/step - loss: 7.8673 - gender_output_loss: 0.6857 - image_quality_output_loss: 0.9805 - age_output_loss: 1.4284 - weight_output_loss: 0.9807 - bag_output_loss: 0.9156 - footwear_output_loss: 1.0409 - pose_output_loss: 0.9280 - emotion_output_loss: 0.9075 - gender_output_acc: 0.5611 - image_quality_output_acc: 0.5543 - age_output_acc: 0.3975 - weight_output_acc: 0.6354 - bag_output_acc: 0.5661 - footwear_output_acc: 0.4448 - pose_output_acc: 0.6166 - emotion_output_acc: 0.7109 - val_loss: 7.8631 - val_gender_output_loss: 0.6835 - val_image_quality_output_loss: 0.9906 - val_age_output_loss: 1.4260 - val_weight_output_loss: 0.9856 - val_bag_output_loss: 0.9266 - val_footwear_output_loss: 1.0381 - val_pose_output_loss: 0.9184 - val_emotion_output_loss: 0.8942 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4052 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4456 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "Epoch 49/100\n",
            "360/360 [==============================] - 151s 419ms/step - loss: 7.8670 - gender_output_loss: 0.6857 - image_quality_output_loss: 0.9804 - age_output_loss: 1.4284 - weight_output_loss: 0.9806 - bag_output_loss: 0.9154 - footwear_output_loss: 1.0407 - pose_output_loss: 0.9276 - emotion_output_loss: 0.9082 - gender_output_acc: 0.5614 - image_quality_output_acc: 0.5546 - age_output_acc: 0.3975 - weight_output_acc: 0.6353 - bag_output_acc: 0.5662 - footwear_output_acc: 0.4449 - pose_output_acc: 0.6166 - emotion_output_acc: 0.7107 - val_loss: 7.8651 - val_gender_output_loss: 0.6835 - val_image_quality_output_loss: 0.9903 - val_age_output_loss: 1.4272 - val_weight_output_loss: 0.9866 - val_bag_output_loss: 0.9262 - val_footwear_output_loss: 1.0382 - val_pose_output_loss: 0.9187 - val_emotion_output_loss: 0.8944 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4052 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4456 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "Epoch 50/100\n",
            "360/360 [==============================] - 151s 419ms/step - loss: 7.8666 - gender_output_loss: 0.6857 - image_quality_output_loss: 0.9802 - age_output_loss: 1.4279 - weight_output_loss: 0.9810 - bag_output_loss: 0.9160 - footwear_output_loss: 1.0403 - pose_output_loss: 0.9275 - emotion_output_loss: 0.9081 - gender_output_acc: 0.5615 - image_quality_output_acc: 0.5545 - age_output_acc: 0.3976 - weight_output_acc: 0.6354 - bag_output_acc: 0.5657 - footwear_output_acc: 0.4449 - pose_output_acc: 0.6168 - emotion_output_acc: 0.7108 - val_loss: 7.8646 - val_gender_output_loss: 0.6834 - val_image_quality_output_loss: 0.9919 - val_age_output_loss: 1.4263 - val_weight_output_loss: 0.9857 - val_bag_output_loss: 0.9261 - val_footwear_output_loss: 1.0383 - val_pose_output_loss: 0.9190 - val_emotion_output_loss: 0.8940 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4052 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4456 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "\n",
            "Epoch 00050: saving model to /content/gdrive/My Drive/person_attributes_densenet_augment.hdf5\n",
            "Epoch 51/100\n",
            "360/360 [==============================] - 150s 417ms/step - loss: 7.8669 - gender_output_loss: 0.6857 - image_quality_output_loss: 0.9806 - age_output_loss: 1.4282 - weight_output_loss: 0.9808 - bag_output_loss: 0.9164 - footwear_output_loss: 1.0408 - pose_output_loss: 0.9273 - emotion_output_loss: 0.9072 - gender_output_acc: 0.5612 - image_quality_output_acc: 0.5540 - age_output_acc: 0.3975 - weight_output_acc: 0.6355 - bag_output_acc: 0.5656 - footwear_output_acc: 0.4449 - pose_output_acc: 0.6167 - emotion_output_acc: 0.7111 - val_loss: 7.8651 - val_gender_output_loss: 0.6835 - val_image_quality_output_loss: 0.9910 - val_age_output_loss: 1.4252 - val_weight_output_loss: 0.9862 - val_bag_output_loss: 0.9265 - val_footwear_output_loss: 1.0383 - val_pose_output_loss: 0.9189 - val_emotion_output_loss: 0.8955 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4052 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4456 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "Epoch 52/100\n",
            "360/360 [==============================] - 151s 419ms/step - loss: 7.8682 - gender_output_loss: 0.6857 - image_quality_output_loss: 0.9805 - age_output_loss: 1.4286 - weight_output_loss: 0.9815 - bag_output_loss: 0.9155 - footwear_output_loss: 1.0406 - pose_output_loss: 0.9278 - emotion_output_loss: 0.9080 - gender_output_acc: 0.5614 - image_quality_output_acc: 0.5543 - age_output_acc: 0.3974 - weight_output_acc: 0.6349 - bag_output_acc: 0.5660 - footwear_output_acc: 0.4450 - pose_output_acc: 0.6167 - emotion_output_acc: 0.7110 - val_loss: 7.8626 - val_gender_output_loss: 0.6833 - val_image_quality_output_loss: 0.9902 - val_age_output_loss: 1.4251 - val_weight_output_loss: 0.9858 - val_bag_output_loss: 0.9264 - val_footwear_output_loss: 1.0382 - val_pose_output_loss: 0.9193 - val_emotion_output_loss: 0.8943 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4052 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4456 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "Epoch 53/100\n",
            "360/360 [==============================] - 151s 420ms/step - loss: 7.8678 - gender_output_loss: 0.6857 - image_quality_output_loss: 0.9807 - age_output_loss: 1.4283 - weight_output_loss: 0.9812 - bag_output_loss: 0.9156 - footwear_output_loss: 1.0406 - pose_output_loss: 0.9277 - emotion_output_loss: 0.9080 - gender_output_acc: 0.5612 - image_quality_output_acc: 0.5541 - age_output_acc: 0.3975 - weight_output_acc: 0.6352 - bag_output_acc: 0.5660 - footwear_output_acc: 0.4447 - pose_output_acc: 0.6166 - emotion_output_acc: 0.7107 - val_loss: 7.8626 - val_gender_output_loss: 0.6833 - val_image_quality_output_loss: 0.9902 - val_age_output_loss: 1.4249 - val_weight_output_loss: 0.9856 - val_bag_output_loss: 0.9277 - val_footwear_output_loss: 1.0382 - val_pose_output_loss: 0.9182 - val_emotion_output_loss: 0.8944 - val_gender_output_acc: 0.5706 - val_image_quality_output_acc: 0.5479 - val_age_output_acc: 0.4052 - val_weight_output_acc: 0.6411 - val_bag_output_acc: 0.5479 - val_footwear_output_acc: 0.4456 - val_pose_output_acc: 0.6250 - val_emotion_output_acc: 0.7182\n",
            "Epoch 54/100\n",
            "346/360 [===========================>..] - ETA: 5s - loss: 7.8683 - gender_output_loss: 0.6859 - image_quality_output_loss: 0.9797 - age_output_loss: 1.4284 - weight_output_loss: 0.9829 - bag_output_loss: 0.9177 - footwear_output_loss: 1.0401 - pose_output_loss: 0.9251 - emotion_output_loss: 0.9084 - gender_output_acc: 0.5602 - image_quality_output_acc: 0.5546 - age_output_acc: 0.3979 - weight_output_acc: 0.6348 - bag_output_acc: 0.5644 - footwear_output_acc: 0.4449 - pose_output_acc: 0.6187 - emotion_output_acc: 0.7106"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Process ForkPoolWorker-791:\n",
            "Process ForkPoolWorker-789:\n",
            "Process ForkPoolWorker-795:\n",
            "Process ForkPoolWorker-794:\n",
            "Process ForkPoolWorker-792:\n",
            "Process ForkPoolWorker-788:\n",
            "Process ForkPoolWorker-787:\n",
            "Process ForkPoolWorker-786:\n",
            "Process ForkPoolWorker-796:\n",
            "Process ForkPoolWorker-790:\n",
            "Process ForkPoolWorker-793:\n",
            "Traceback (most recent call last):\n",
            "Process ForkPoolWorker-797:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
            "    res = self._reader.recv_bytes()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
            "    res = self._reader.recv_bytes()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-64fab3828b18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    214\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1448\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1449\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1450\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zI1hJb4qM6OH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}